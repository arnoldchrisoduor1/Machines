{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3dce158-9cc7-47f9-9ea4-07e33669f54a",
   "metadata": {},
   "source": [
    "# Building Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb2bef3-0e35-4090-840f-3fd82efeb602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ffd936-4870-4793-8070-c039c498454b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x2b7b38cc810>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    " \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    " \"the-verdict.txt\")\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8285e637-17b2-424e-89c1-0bed693197e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character: \", len(raw_text))\n",
    "print(raw_text[:98])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e648a64-92ba-42b3-a26f-aaa924a1a728",
   "metadata": {},
   "source": [
    "# Using re to split text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa82a3f-3359-40f7-9640-84b786e9d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64c8ef6-27b5-4fbb-9d3b-e270a3f2d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. This is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757d7858-9315-4d5f-8392-2f0504af54c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'world.', 'This', 'is', 'a', 'test.']\n"
     ]
    }
   ],
   "source": [
    "# Removing all the white spaces.\n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf62e84-4fdd-4117-94a8-688537d5e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'iS', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "# Modifying the tokenizer to handle more things such as punctuations etc.\n",
    "\n",
    "text = \"Hello, world. iS this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "reult = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c992a8d-f4f5-44ab-8450-44d81c41c6c2",
   "metadata": {},
   "source": [
    "## Applying this working tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91af0708-6cd6-4239-8f88-5e1f96ce7fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8738d652-f6e9-409d-9ed2-18563b6965c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d719f6c-4b95-4a60-8e3a-b13f7511847d",
   "metadata": {},
   "source": [
    "## Converting Tokens into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1d8397-71fb-4851-b149-ecc69534c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# Converting the tokens from python strings to integer representations to produce\n",
    "#token IDs.\n",
    "\n",
    "# We'll create a list of all unique tokens and sort them.\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ed8583f-e9dc-47e8-a4a2-49c522967754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee03581-9cd0-4daf-a30e-64798972be39",
   "metadata": {},
   "source": [
    "# Implementing the full text Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a846eb5-2232-4b93-9c6a-ebd2da50a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    " def __init__(self, vocab):\n",
    "     self.str_to_int = vocab\n",
    "     self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    " def encode(self, text):\n",
    "     preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "     preprocessed = [\n",
    "     item.strip() for item in preprocessed if item.strip()\n",
    "     ]\n",
    "     ids = [self.str_to_int[s] for s in preprocessed]\n",
    "     return ids\n",
    "\n",
    " def decode(self, ids):\n",
    "     text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    \n",
    "     text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "804f4744-b590-41c2-ba6d-94b0b54ee394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    " Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41c3015-1b27-4f17-b710-445176ef7c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3648a66-745c-4039-b51f-4c2e2463e442",
   "metadata": {},
   "source": [
    "# Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "470cca69-b0f1-4398-aec2-ff3fb7043749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d52d1c17-16aa-43c8-8646-9f3e71228114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# Printing the last 5 entries of the updated vocabulary.\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cd697-17a9-42be-a47b-39df77afbdcd",
   "metadata": {},
   "source": [
    "## Modified tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e101c8-e48e-443e-8b26-459c8c3d53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    " def __init__(self, vocab):\n",
    "     self.str_to_int = vocab\n",
    "     self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    " def encode(self, text):\n",
    "     preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "     preprocessed = [\n",
    "     item.strip() for item in preprocessed if item.strip()\n",
    "     ]\n",
    "     preprocessed = [item if item in self.str_to_int\n",
    "     else \"<|unk|>\" for item in preprocessed]\n",
    "     ids = [self.str_to_int[s] for s in preprocessed]\n",
    "     return ids\n",
    "\n",
    " def decode(self, ids):\n",
    "     text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "     text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a16507-b1e5-46df-ada9-ef831330a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14c49d-d8fd-49f9-8dac-425b7c366bc3",
   "metadata": {},
   "source": [
    "# Data Sampling with a Sliding Window\n",
    " -  Implementing the loader wthat will create the input-target pairs from the training\n",
    " -  dataset using the sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcff1475-d3d3-478a-8cad-994b85eed0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the whole verdict story.\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c904b63a-fb6c-46ce-8ccc-1710323ba54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a15e3e8-3235-428a-8874-6e5e2ec8db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the first 50 tokens for testing - results in better subsequent predictions.\n",
    "\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "490f97a3-f4bf-494c-a969-0b75ef66d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [568, 115, 1066, 727]\n",
      "y: [115, 1066, 727, 988]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e46ba00e-bc5e-4037-98a5-f166829669aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[568] ------> 115\n",
      "[568, 115] ------> 1066\n",
      "[568, 115, 1066] ------> 727\n",
      "[568, 115, 1066, 727] ------> 988\n"
     ]
    }
   ],
   "source": [
    "# example of the next word prediction tassks.\n",
    "\n",
    "for i in range (1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"------>\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2210c-5e8c-41cc-931b-1ada5645ec5f",
   "metadata": {},
   "source": [
    "# Implementing and efficient dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72c2502a-0411-4293-a3ec-10a4f7f7c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15f189b2-141d-4198-9e6a-53c739ce159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "             input_chunk = token_ids[i:i + max_length]\n",
    "             target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "             self.input_ids.append(torch.tensor(input_chunk))\n",
    "             self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d08a87e-8dff-48b0-a659-94cdcc843ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will load the inputs in batches using the PyTorch DataLoader.\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=shuffle, \n",
    "                            drop_last=drop_last, \n",
    "                            num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a345bd5-0aec-404d-b98b-322a9426cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Testing with a batch size of 1 context size of 4.\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_test = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861d39c-2c4d-4599-aaab-ebfeb31df783",
   "metadata": {},
   "source": [
    "# Creating the token Embeddings\n",
    " - Last step in preparing the input text is converting the tokekIDs into embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6a351b6-63dd-4662-8915-5ee619c1179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccf6e65a-2954-4af8-9d38-450ee26d894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbb863b3-2bbd-4fd9-ab76-f7f07d48bb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4dc4fe62-09b1-49fd-a4ea-4cbfdf22aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18da8f-cbc8-4919-b818-7ffce309ef83",
   "metadata": {},
   "source": [
    "# Encoding Word Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "528b7db2-7d4e-4534-9265-509d9d6d6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encode the positional data so the model learns about the varying positions of embedding in the data.\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0b9be4a-ce3c-4933-83f6-188c4dd58868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, \n",
    "    max_length=max_length,\n",
    "    stride = max_length, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efb4546c-dd37-4751-9e4e-f923d282ba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Using the embedding layer to embed these token IDs into 256-dimensional vectors.\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "295f6e5f-8066-4650-842f-d0d7088a91d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e0c56-f852-4835-bb53-e29931264ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
