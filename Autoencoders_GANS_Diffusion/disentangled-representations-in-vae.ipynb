{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Loading and Preprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\n\n# Step 1: Custom Dataset Class for CelebA\nclass CelebADataset(Dataset):\n    def __init__(self, img_dir, attr_file, transform=None, selected_attrs=None):\n        \"\"\"\n        Custom dataset for CelebA that combines images with attributes\n        \n        Args:\n            img_dir: Directory containing images\n            attr_file: Path to attributes CSV file (usually list_attr_celeba.csv)\n            transform: Image transformations\n            selected_attrs: List of attribute names to use (None = use all)\n        \"\"\"\n        self.img_dir = img_dir\n        self.transform = transform\n        \n        # Load attributes CSV\n        # The CSV typically has image names in first column and attributes as other columns\n        self.attr_df = pd.read_csv(attr_file)\n        \n        # Handle different CSV formats - some have image names as index, some as first column\n        if 'image_id' not in self.attr_df.columns and self.attr_df.columns[0] != 'image_id':\n            # If first column contains image names but isn't named 'image_id'\n            self.attr_df.rename(columns={self.attr_df.columns[0]: 'image_id'}, inplace=True)\n        \n        # Get list of image files that exist\n        available_images = set(os.listdir(img_dir))\n        \n        # Filter dataframe to only include images that exist\n        self.attr_df = self.attr_df[self.attr_df['image_id'].isin(available_images)]\n        \n        # Select specific attributes if provided\n        if selected_attrs:\n            cols_to_keep = ['image_id'] + selected_attrs\n            self.attr_df = self.attr_df[cols_to_keep]\n        \n        # Convert attribute values to 0/1 (they might be -1/1 in original)\n        attr_cols = [col for col in self.attr_df.columns if col != 'image_id']\n        for col in attr_cols:\n            self.attr_df[col] = (self.attr_df[col] + 1) / 2  # Convert -1,1 to 0,1\n        \n        self.attr_names = attr_cols\n        \n        print(f\"Loaded {len(self.attr_df)} images with {len(self.attr_names)} attributes\")\n        print(f\"Attributes: {self.attr_names}\")\n    \n    def __len__(self):\n        return len(self.attr_df)\n    \n    def __getitem__(self, idx):\n        # Get image path and attributes\n        row = self.attr_df.iloc[idx]\n        img_name = row['image_id']\n        img_path = os.path.join(self.img_dir, img_name)\n        \n        # Load image\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            # Return a black image as fallback\n            image = Image.new('RGB', (178, 218), color='black')\n        \n        # Apply transforms\n        if self.transform:\n            image = self.transform(image)\n        \n        # Get attributes (excluding image_id)\n        attributes = torch.tensor(row[self.attr_names].astype(int).values, dtype=torch.float32)\n        \n        return image, attributes, img_name\n\n# Step 2: Data Loading Setup\ndef setup_celeba_data(img_dir, attr_file, batch_size=32, img_size=64, selected_attrs=None):\n    \"\"\"\n    Set up data loaders for CelebA dataset\n    \n    Args:\n        img_dir: Path to images directory\n        attr_file: Path to attributes CSV file\n        batch_size: Batch size for training\n        img_size: Size to resize images to\n        selected_attrs: List of specific attributes to use\n    \"\"\"\n    \n    # Define transforms\n    transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n    ])\n    \n    # Create dataset\n    dataset = CelebADataset(img_dir, attr_file, transform, selected_attrs)\n    \n    # Create data loader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    \n    return dataset, dataloader\n\n# Step 3: Example usage and data exploration\ndef explore_celeba_data(dataset, dataloader):\n    \"\"\"\n    Explore the loaded dataset\n    \"\"\"\n    print(f\"Dataset size: {len(dataset)}\")\n    print(f\"Number of attributes: {len(dataset.attr_names)}\")\n    print(f\"Attribute names: {dataset.attr_names}\")\n    \n    # Get a sample batch\n    sample_batch = next(iter(dataloader))\n    images, attributes, img_names = sample_batch\n    \n    print(f\"Batch image shape: {images.shape}\")\n    print(f\"Batch attributes shape: {attributes.shape}\")\n    print(f\"Sample image names: {img_names[:5]}\")\n    \n    # Show attribute statistics\n    print(\"\\nAttribute statistics (first batch):\")\n    for i, attr_name in enumerate(dataset.attr_names):\n        mean_val = attributes[:, i].mean().item()\n        print(f\"{attr_name}: {mean_val:.2f}\")\n    \n    return sample_batch\n\n# Usage example:\n\n# Set paths according to your Kaggle dataset structure\nIMG_DIR = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\nATTR_FILE = \"/kaggle/input/celeba-dataset/list_attr_celeba.csv\"\n\n# You can select specific attributes for disentanglement experiments\nSELECTED_ATTRS = [\n    'Male', 'Young', 'Eyeglasses', 'Bald', 'Mustache', \n    'Smiling', 'Attractive', 'Blond_Hair', 'Heavy_Makeup'\n]\n\n# Or use all attributes by setting SELECTED_ATTRS = None\n\n# Create dataset and dataloader\ndataset, dataloader = setup_celeba_data(\n    IMG_DIR, ATTR_FILE, \n    batch_size=32, \n    img_size=64, \n    selected_attrs=SELECTED_ATTRS\n)\n\n# Explore the data\nsample_batch = explore_celeba_data(dataset, dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T12:15:45.656034Z","iopub.execute_input":"2025-08-20T12:15:45.657878Z","iopub.status.idle":"2025-08-20T12:15:50.058509Z","shell.execute_reply.started":"2025-08-20T12:15:45.657819Z","shell.execute_reply":"2025-08-20T12:15:50.057209Z"}},"outputs":[{"name":"stdout","text":"Loaded 202599 images with 9 attributes\nAttributes: ['Male', 'Young', 'Eyeglasses', 'Bald', 'Mustache', 'Smiling', 'Attractive', 'Blond_Hair', 'Heavy_Makeup']\nDataset size: 202599\nNumber of attributes: 9\nAttribute names: ['Male', 'Young', 'Eyeglasses', 'Bald', 'Mustache', 'Smiling', 'Attractive', 'Blond_Hair', 'Heavy_Makeup']\nBatch image shape: torch.Size([32, 3, 64, 64])\nBatch attributes shape: torch.Size([32, 9])\nSample image names: ('085276.jpg', '075760.jpg', '186410.jpg', '189671.jpg', '169024.jpg')\n\nAttribute statistics (first batch):\nMale: 0.53\nYoung: 0.72\nEyeglasses: 0.09\nBald: 0.03\nMustache: 0.06\nSmiling: 0.44\nAttractive: 0.44\nBlond_Hair: 0.12\nHeavy_Makeup: 0.25\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# VAE architecture with CNNs","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Step 2: VAE Architecture with CNNs\n\nclass Encoder(nn.Module):\n    def __init__(self, latent_dim=64, img_channels=3):\n        super(Encoder, self).__init__()\n        self.latent_dim = latent_dim\n        \n        # CNN layers for feature extraction\n        self.conv_layers = nn.Sequential(\n            # Input: 3 x 64 x 64\n            nn.Conv2d(img_channels, 32, kernel_size=4, stride=2, padding=1),  # 32 x 32 x 32\n            nn.BatchNorm2d(32),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 64 x 16 x 16\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 128 x 8 x 8\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 256 x 4 x 4\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        \n        # Calculate flattened size\n        self.flattened_size = 256 * 4 * 4\n        \n        # Fully connected layers for latent space\n        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)\n        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)\n        \n    def forward(self, x):\n        # CNN feature extraction\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        \n        # Get latent parameters\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        \n        return mu, logvar\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim=64, img_channels=3):\n        super(Decoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.img_channels = img_channels\n        \n        # Project latent to feature map\n        self.fc = nn.Linear(latent_dim, 256 * 4 * 4)\n        \n        # Transpose CNN layers for image generation\n        self.deconv_layers = nn.Sequential(\n            # 256 x 4 x 4\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 128 x 8 x 8\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 64 x 16 x 16\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 32 x 32 x 32\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(32, img_channels, kernel_size=4, stride=2, padding=1),  # 3 x 64 x 64\n            nn.Tanh()  # Output in [-1, 1] range\n        )\n        \n    def forward(self, z):\n        # Project and reshape\n        x = self.fc(z)\n        x = x.view(x.size(0), 256, 4, 4)\n        \n        # Generate image\n        x = self.deconv_layers(x)\n        \n        return x\n\nclass VAE(nn.Module):\n    def __init__(self, latent_dim=64, img_channels=3):\n        super(VAE, self).__init__()\n        self.latent_dim = latent_dim\n        \n        self.encoder = Encoder(latent_dim, img_channels)\n        self.decoder = Decoder(latent_dim, img_channels)\n        \n    def reparameterize(self, mu, logvar):\n        \"\"\"\n        Reparameterization trick for VAE\n        \"\"\"\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return mu + eps * std\n        else:\n            return mu\n    \n    def forward(self, x):\n        # Encode\n        mu, logvar = self.encoder(x)\n        \n        # Reparameterize\n        z = self.reparameterize(mu, logvar)\n        \n        # Decode\n        recon_x = self.decoder(z)\n        \n        return recon_x, mu, logvar, z\n    \n    def generate(self, num_samples, device):\n        \"\"\"\n        Generate new samples from the latent space\n        \"\"\"\n        self.eval()\n        with torch.no_grad():\n            z = torch.randn(num_samples, self.latent_dim).to(device)\n            generated = self.decoder(z)\n        return generated\n    \n    def encode(self, x):\n        \"\"\"\n        Encode input to latent space\n        \"\"\"\n        self.eval()\n        with torch.no_grad():\n            mu, logvar = self.encoder(x)\n            z = self.reparameterize(mu, logvar)\n        return z, mu, logvar\n\n# Test the architecture\ndef test_vae_architecture():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Create VAE\n    vae = VAE(latent_dim=64, img_channels=3).to(device)\n    \n    # Test with dummy input\n    batch_size = 8\n    dummy_input = torch.randn(batch_size, 3, 64, 64).to(device)\n    \n    print(\"Testing VAE architecture...\")\n    print(f\"Input shape: {dummy_input.shape}\")\n    \n    # Forward pass\n    recon_x, mu, logvar, z = vae(dummy_input)\n    \n    print(f\"Reconstructed shape: {recon_x.shape}\")\n    print(f\"Latent z shape: {z.shape}\")\n    print(f\"Mu shape: {mu.shape}\")\n    print(f\"Logvar shape: {logvar.shape}\")\n    \n    # Test generation\n    generated = vae.generate(num_samples=4, device=device)\n    print(f\"Generated samples shape: {generated.shape}\")\n    \n    # Count parameters\n    total_params = sum(p.numel() for p in vae.parameters())\n    trainable_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n    \n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    return vae\n\n# Usage:\nvae_model = test_vae_architecture()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T12:15:50.060839Z","iopub.execute_input":"2025-08-20T12:15:50.061178Z","iopub.status.idle":"2025-08-20T12:15:50.176551Z","shell.execute_reply.started":"2025-08-20T12:15:50.061144Z","shell.execute_reply":"2025-08-20T12:15:50.175251Z"}},"outputs":[{"name":"stdout","text":"Testing VAE architecture...\nInput shape: torch.Size([8, 3, 64, 64])\nReconstructed shape: torch.Size([8, 3, 64, 64])\nLatent z shape: torch.Size([8, 64])\nMu shape: torch.Size([8, 64])\nLogvar shape: torch.Size([8, 64])\nGenerated samples shape: torch.Size([4, 3, 64, 64])\nTotal parameters: 2,172,099\nTrainable parameters: 2,172,099\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Disentangled VAE Loss Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\n\n# Step 3: Loss Functions for Disentangled VAE\n\ndef vae_loss(recon_x, x, mu, logvar, beta=1.0):\n    \"\"\"\n    Standard VAE loss with β-VAE modification\n    \n    Args:\n        recon_x: Reconstructed images\n        x: Original images\n        mu: Mean of latent distribution\n        logvar: Log variance of latent distribution\n        beta: Weight for KL divergence (β-VAE)\n    \"\"\"\n    # Reconstruction loss (MSE or BCE)\n    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n    \n    # KL divergence loss\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    \n    # Total loss\n    total_loss = recon_loss + beta * kl_loss\n    \n    return total_loss, recon_loss, kl_loss\n\ndef factor_vae_loss(recon_x, x, mu, logvar, z, discriminator=None, gamma=10.0, beta=1.0):\n    \"\"\"\n    Factor-VAE loss for disentanglement\n    \n    Args:\n        recon_x: Reconstructed images\n        x: Original images\n        mu: Mean of latent distribution\n        logvar: Log variance of latent distribution\n        z: Sampled latent codes\n        discriminator: Discriminator network (if available)\n        gamma: Weight for total correlation penalty\n        beta: Weight for KL divergence\n    \"\"\"\n    # Standard VAE loss\n    vae_loss_val, recon_loss, kl_loss = vae_loss(recon_x, x, mu, logvar, beta)\n    \n    # Total Correlation penalty (if discriminator is available)\n    tc_loss = 0\n    if discriminator is not None:\n        # Permute latent codes to break dependencies\n        z_perm = permute_latent_codes(z)\n        \n        # Discriminator scores\n        d_z = discriminator(z)\n        d_z_perm = discriminator(z_perm)\n        \n        # Total correlation loss\n        tc_loss = torch.mean(d_z) - torch.mean(d_z_perm)\n    \n    total_loss = vae_loss_val + gamma * tc_loss\n    \n    return total_loss, recon_loss, kl_loss, tc_loss\n\ndef beta_tcvae_loss(recon_x, x, mu, logvar, z, alpha=1.0, beta=6.0, gamma=1.0):\n    batch_size, latent_dim = z.shape\n    \n    # Reconstruction loss\n    recon_loss = F.mse_loss(recon_x, x, reduction='sum') / batch_size\n    \n    # log q(z|x) = sum over latent dims\n    log_qz_cond = log_density_gaussian(z, mu, logvar).sum(1)  # [batch]\n    log_pz = log_density_standard_gaussian(z).sum(1)          # [batch]\n    \n    # Pairwise comparisons for marginal q(z)\n    z_expand = z.unsqueeze(1)          # [B, 1, L]\n    mu_expand = mu.unsqueeze(0)        # [1, B, L]\n    logvar_expand = logvar.unsqueeze(0)# [1, B, L]\n\n    # log q(z_j | x_i) for all pairs\n    log_qz_matrix = log_density_gaussian(z_expand, mu_expand, logvar_expand)  # [B, B, L]\n    log_qz = torch.logsumexp(log_qz_matrix.sum(2), dim=1) - np.log(batch_size)  # [B]\n\n    # MI, TC, DW (see Chen et al., β-TCVAE)\n    mi_loss = (log_qz_cond - log_qz).mean()\n    tc_loss = (log_qz - log_qz_matrix.sum(2).mean(0)).mean()\n    dw_kl_loss = (log_qz_matrix.sum(2).mean(0) - log_pz.mean()).mean()\n\n    total_loss = recon_loss + alpha * mi_loss + beta * tc_loss + gamma * dw_kl_loss\n    return total_loss, recon_loss, mi_loss, tc_loss, dw_kl_loss\n    \n\ndef log_density_gaussian(z, mu, logvar):\n    \"\"\"\n    Log density of Gaussian N(mu, sigma^2) for each dimension.\n    Returns shape [batch, latent_dim].\n    \"\"\"\n    norm_const = -0.5 * np.log(2 * np.pi)\n    log_density = norm_const - 0.5 * logvar - 0.5 * ((z - mu) ** 2) / torch.exp(logvar)\n    return log_density  # [batch, latent_dim]\n\ndef log_density_standard_gaussian(z):\n    \"\"\"\n    Log density of standard Gaussian N(0, I).\n    Returns shape [batch, latent_dim].\n    \"\"\"\n    norm_const = -0.5 * np.log(2 * np.pi)\n    log_density = norm_const - 0.5 * (z ** 2)\n    return log_density  # [batch, latent_dim]\n\ndef permute_latent_codes(z):\n    \"\"\"\n    Randomly permute latent codes across batch dimension\n    \"\"\"\n    z_perm = z.clone()\n    for i in range(z.size(1)):  # For each latent dimension\n        perm_idx = torch.randperm(z.size(0))\n        z_perm[:, i] = z_perm[perm_idx, i]\n    return z_perm\n\ndef supervised_disentanglement_loss(z, attributes, attribute_weights=None):\n    \"\"\"\n    Supervised loss to encourage specific latent dimensions to correspond to attributes\n    \n    Args:\n        z: Latent codes [batch_size, latent_dim]\n        attributes: Ground truth attributes [batch_size, num_attributes]\n        attribute_weights: Weights for each attribute\n    \"\"\"\n    batch_size, latent_dim = z.shape\n    batch_size, num_attributes = attributes.shape\n    \n    if attribute_weights is None:\n        attribute_weights = torch.ones(num_attributes)\n    \n    # Simple approach: use first few latent dimensions for attributes\n    num_supervised = min(latent_dim, num_attributes)\n    \n    # Supervised loss: encourage z[:, :num_supervised] to predict attributes\n    z_attr = z[:, :num_supervised]\n    attr_target = attributes[:, :num_supervised]\n    \n    # Use sigmoid to get probabilities and BCE loss\n    attr_pred = torch.sigmoid(z_attr)\n    supervised_loss = F.binary_cross_entropy(attr_pred, attr_target, weight=attribute_weights[:num_supervised])\n    \n    return supervised_loss\n\n# Complete loss function combining different approaches\nclass DisentangledVAELoss:\n    def __init__(self, loss_type='beta_vae', beta=4.0, gamma=10.0, alpha=1.0, \n                 supervised_weight=0.1):\n        self.loss_type = loss_type\n        self.beta = beta\n        self.gamma = gamma\n        self.alpha = alpha\n        self.supervised_weight = supervised_weight\n        \n    def __call__(self, recon_x, x, mu, logvar, z, attributes=None):\n        \"\"\"\n        Compute disentangled VAE loss\n        \"\"\"\n        losses = {}\n        \n        if self.loss_type == 'beta_vae':\n            total_loss, recon_loss, kl_loss = vae_loss(recon_x, x, mu, logvar, self.beta)\n            losses.update({\n                'total': total_loss,\n                'reconstruction': recon_loss,\n                'kl': kl_loss\n            })\n            \n        elif self.loss_type == 'beta_tcvae':\n            total_loss, recon_loss, mi_loss, tc_loss, dw_kl_loss = beta_tcvae_loss(\n                recon_x, x, mu, logvar, z, self.alpha, self.beta, self.gamma\n            )\n            losses.update({\n                'total': total_loss,\n                'reconstruction': recon_loss,\n                'mi': mi_loss,\n                'tc': tc_loss,\n                'dw_kl': dw_kl_loss\n            })\n        \n        # Add supervised disentanglement if attributes provided\n        if attributes is not None and self.supervised_weight > 0:\n            supervised_loss = supervised_disentanglement_loss(z, attributes)\n            losses['supervised'] = supervised_loss\n            losses['total'] = losses['total'] + self.supervised_weight * supervised_loss\n        \n        return losses\n\n# Usage example:\n\n# Create loss function\n# loss_fn = DisentangledVAELoss(\n#     loss_type='beta_tcvae',\n#     beta=6.0,\n#     gamma=1.0,\n#     alpha=1.0,\n#     supervised_weight=0.1\n# )\n\n# # During training:\n# recon_x, mu, logvar, z = vae_model(images)\n# losses = loss_fn(recon_x, images, mu, logvar, z, attributes)\n\n# total_loss = losses['total']\n# total_loss.backward()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T12:15:50.178009Z","iopub.execute_input":"2025-08-20T12:15:50.178329Z","iopub.status.idle":"2025-08-20T12:15:50.203720Z","shell.execute_reply.started":"2025-08-20T12:15:50.178300Z","shell.execute_reply":"2025-08-20T12:15:50.202401Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Complete Training Loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfrom datetime import datetime\n\n# Step 4: Complete Training Loop\n\nclass VAETrainer:\n    def __init__(self, model, train_loader, val_loader=None, device='cuda', \n                 loss_type='beta_tcvae', lr=1e-4, save_dir='./vae_checkpoints'):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.save_dir = save_dir\n        \n        # Create save directory\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Optimizer\n        self.optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n        \n        # Loss function\n        self.loss_fn = DisentangledVAELoss(\n            loss_type=loss_type,\n            beta=6.0 if loss_type == 'beta_tcvae' else 4.0,\n            gamma=1.0,\n            alpha=1.0,\n            supervised_weight=0.1\n        )\n        \n        # Tensorboard writer\n        self.writer = SummaryWriter(f'runs/vae_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n        \n        # Training history\n        self.train_losses = []\n        self.val_losses = []\n        \n    def train_epoch(self, epoch):\n        self.model.train()\n        total_losses = {}\n        num_batches = len(self.train_loader)\n        \n        with tqdm(self.train_loader, desc=f'Epoch {epoch}') as pbar:\n            for batch_idx, (images, attributes, _) in enumerate(pbar):\n                images = images.to(self.device)\n                attributes = attributes.to(self.device)\n                \n                # Forward pass\n                recon_x, mu, logvar, z = self.model(images)\n                \n                # Compute loss\n                losses = self.loss_fn(recon_x, images, mu, logvar, z, attributes)\n                \n                # Backward pass\n                self.optimizer.zero_grad()\n                losses['total'].backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                \n                self.optimizer.step()\n                \n                # Update running losses\n                for key, value in losses.items():\n                    if key not in total_losses:\n                        total_losses[key] = 0\n                    total_losses[key] += value.item()\n                \n                # Update progress bar\n                pbar.set_postfix({\n                    'Total': f\"{losses['total'].item():.4f}\",\n                    'Recon': f\"{losses['reconstruction'].item():.4f}\",\n                    'KL': f\"{losses.get('kl', losses.get('tc', 0)):.4f}\"\n                })\n                \n                # Log to tensorboard\n                global_step = epoch * num_batches + batch_idx\n                if batch_idx % 100 == 0:\n                    for key, value in losses.items():\n                        self.writer.add_scalar(f'Train/{key}', value.item(), global_step)\n        \n        # Average losses\n        avg_losses = {key: value / num_batches for key, value in total_losses.items()}\n        self.train_losses.append(avg_losses)\n        \n        return avg_losses\n    \n    def validate(self, epoch):\n        if self.val_loader is None:\n            return None\n            \n        self.model.eval()\n        total_losses = {}\n        num_batches = len(self.val_loader)\n        \n        with torch.no_grad():\n            for images, attributes, _ in self.val_loader:\n                images = images.to(self.device)\n                attributes = attributes.to(self.device)\n                \n                # Forward pass\n                recon_x, mu, logvar, z = self.model(images)\n                \n                # Compute loss\n                losses = self.loss_fn(recon_x, images, mu, logvar, z, attributes)\n                \n                # Update running losses\n                for key, value in losses.items():\n                    if key not in total_losses:\n                        total_losses[key] = 0\n                    total_losses[key] += value.item()\n        \n        # Average losses\n        avg_losses = {key: value / num_batches for key, value in total_losses.items()}\n        self.val_losses.append(avg_losses)\n        \n        # Log to tensorboard\n        for key, value in avg_losses.items():\n            self.writer.add_scalar(f'Val/{key}', value, epoch)\n        \n        return avg_losses\n    \n    def save_samples(self, epoch, num_samples=8):\n        \"\"\"Save reconstructed and generated samples\"\"\"\n        self.model.eval()\n        \n        with torch.no_grad():\n            # Get a batch for reconstruction\n            images, _, _ = next(iter(self.train_loader))\n            images = images[:num_samples].to(self.device)\n            \n            # Reconstruct\n            recon_images, _, _, _ = self.model(images)\n            \n            # Generate new samples\n            generated_images = self.model.generate(num_samples, self.device)\n            \n            # Denormalize images for visualization\n            def denormalize(x):\n                return (x + 1) / 2  # From [-1, 1] to [0, 1]\n            \n            # Create comparison plot\n            fig, axes = plt.subplots(3, num_samples, figsize=(num_samples * 2, 6))\n            \n            for i in range(num_samples):\n                # Original\n                orig_img = denormalize(images[i]).cpu().permute(1, 2, 0).numpy()\n                axes[0, i].imshow(orig_img)\n                axes[0, i].set_title('Original')\n                axes[0, i].axis('off')\n                \n                # Reconstructed\n                recon_img = denormalize(recon_images[i]).cpu().permute(1, 2, 0).numpy()\n                axes[1, i].imshow(recon_img)\n                axes[1, i].set_title('Reconstructed')\n                axes[1, i].axis('off')\n                \n                # Generated\n                gen_img = denormalize(generated_images[i]).cpu().permute(1, 2, 0).numpy()\n                axes[2, i].imshow(gen_img)\n                axes[2, i].set_title('Generated')\n                axes[2, i].axis('off')\n            \n            plt.tight_layout()\n            plt.savefig(f'{self.save_dir}/samples_epoch_{epoch}.png')\n            plt.close()\n    \n    def save_checkpoint(self, epoch, best_loss=None):\n        \"\"\"Save model checkpoint\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'train_losses': self.train_losses,\n            'val_losses': self.val_losses,\n            'best_loss': best_loss\n        }\n        \n        torch.save(checkpoint, f'{self.save_dir}/checkpoint_epoch_{epoch}.pth')\n        torch.save(checkpoint, f'{self.save_dir}/latest_checkpoint.pth')\n        \n        if best_loss is not None:\n            torch.save(checkpoint, f'{self.save_dir}/best_checkpoint.pth')\n    \n    def load_checkpoint(self, checkpoint_path):\n        \"\"\"Load model checkpoint\"\"\"\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        \n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.train_losses = checkpoint['train_losses']\n        self.val_losses = checkpoint['val_losses']\n        \n        return checkpoint['epoch'], checkpoint.get('best_loss')\n    \n    def train(self, num_epochs, save_every=5, sample_every=5):\n        \"\"\"Main training loop\"\"\"\n        print(f\"Starting training for {num_epochs} epochs...\")\n        print(f\"Device: {self.device}\")\n        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n        \n        best_val_loss = float('inf')\n        \n        for epoch in range(1, num_epochs + 1):\n            # Train\n            train_losses = self.train_epoch(epoch)\n            \n            # Validate\n            val_losses = self.validate(epoch)\n            \n            # Print losses\n            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n            print(f\"Train - Total: {train_losses['total']:.4f}, \"\n                  f\"Recon: {train_losses['reconstruction']:.4f}\")\n            \n            if val_losses:\n                print(f\"Val   - Total: {val_losses['total']:.4f}, \"\n                      f\"Recon: {val_losses['reconstruction']:.4f}\")\n                \n                # Check for best model\n                if val_losses['total'] < best_val_loss:\n                    best_val_loss = val_losses['total']\n                    self.save_checkpoint(epoch, best_val_loss)\n                    print(\"New best model saved!\")\n            \n            # Save samples\n            if epoch % sample_every == 0:\n                self.save_samples(epoch)\n            \n            # Save checkpoint\n            if epoch % save_every == 0:\n                self.save_checkpoint(epoch)\n        \n        print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n        self.writer.close()\n\ndef train_celeba_vae():\n    \"\"\"\n    Complete training pipeline for CelebA VAE\n    \"\"\"\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Data paths (adjust for your Kaggle setup)\n    IMG_DIR = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n    ATTR_FILE = \"/kaggle/input/celeba-dataset/list_attr_celeba.csv\"\n    \n    # Selected attributes for disentanglement\n    SELECTED_ATTRS = [\n        'Male', 'Young', 'Eyeglasses', 'Bald', 'Mustache', \n        'Smiling', 'Attractive', 'Blond_Hair', 'Heavy_Makeup'\n    ]\n    \n    # Hyperparameters\n    BATCH_SIZE = 32\n    IMG_SIZE = 64\n    LATENT_DIM = 64\n    LEARNING_RATE = 1e-4\n    NUM_EPOCHS = 1\n    \n    # Create dataset and dataloader\n    print(\"Loading dataset...\")\n    dataset, train_loader = setup_celeba_data(\n        IMG_DIR, ATTR_FILE,\n        batch_size=BATCH_SIZE,\n        img_size=IMG_SIZE,\n        selected_attrs=SELECTED_ATTRS\n    )\n    \n    # Create validation split (optional)\n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    # Create model\n    print(\"Creating VAE model...\")\n    vae = VAE(latent_dim=LATENT_DIM, img_channels=3).to(device)\n    \n    # Create trainer\n    trainer = VAETrainer(\n        model=vae,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        device=device,\n        loss_type='beta_tcvae',  # or 'beta_vae'\n        lr=LEARNING_RATE,\n        save_dir='/kaggle/working/vae_checkpoints'\n    )\n    \n    # Start training\n    trainer.train(\n        num_epochs=NUM_EPOCHS,\n        save_every=10,\n        sample_every=5\n    )\n    \n    return trainer, vae\n\n# Usage in Kaggle:\n\n# Run this to train the complete VAE\ntrainer, trained_vae = train_celeba_vae()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T12:15:50.206950Z","iopub.execute_input":"2025-08-20T12:15:50.207309Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nLoading dataset...\nLoaded 202599 images with 9 attributes\nAttributes: ['Male', 'Young', 'Eyeglasses', 'Bald', 'Mustache', 'Smiling', 'Attractive', 'Blond_Hair', 'Heavy_Makeup']\nCreating VAE model...\nStarting training for 1 epochs...\nDevice: cpu\nModel parameters: 2,172,099\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 5699/5699 [20:36<00:00,  4.61it/s, Total=1762.1620, Recon=1437.5869, KL=59.3584]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Attribute Manipulation and Evaluation","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom scipy.stats import spearmanr\n\n# Step 5: Attribute Manipulation and Evaluation\n\nclass AttributeManipulator:\n    def __init__(self, vae_model, device='cuda'):\n        self.vae = vae_model\n        self.device = device\n        self.vae.eval()\n        \n        # Store attribute classifiers\n        self.attribute_classifiers = {}\n    \n    def encode_dataset(self, dataloader, max_samples=1000):\n        \"\"\"\n        Encode a dataset to get latent representations and attributes\n        \"\"\"\n        latent_codes = []\n        attributes_list = []\n        \n        with torch.no_grad():\n            sample_count = 0\n            for images, attributes, _ in dataloader:\n                if sample_count >= max_samples:\n                    break\n                    \n                images = images.to(self.device)\n                z, mu, _ = self.vae.encode(images)\n                \n                latent_codes.append(z.cpu().numpy())\n                attributes_list.append(attributes.cpu().numpy())\n                \n                sample_count += len(images)\n        \n        latent_codes = np.vstack(latent_codes)\n        attributes_array = np.vstack(attributes_list)\n        \n        return latent_codes[:max_samples], attributes_array[:max_samples]\n    \n    def train_attribute_classifiers(self, latent_codes, attributes, attribute_names):\n        \"\"\"\n        Train linear classifiers to predict attributes from latent codes\n        \"\"\"\n        print(\"Training attribute classifiers...\")\n        \n        for i, attr_name in enumerate(attribute_names):\n            # Train logistic regression classifier\n            clf = LogisticRegression(random_state=42, max_iter=1000)\n            clf.fit(latent_codes, attributes[:, i])\n            \n            # Evaluate accuracy\n            predictions = clf.predict(latent_codes)\n            accuracy = accuracy_score(attributes[:, i], predictions)\n            \n            self.attribute_classifiers[attr_name] = {\n                'classifier': clf,\n                'accuracy': accuracy\n            }\n            \n            print(f\"{attr_name}: Accuracy = {accuracy:.3f}\")\n    \n    def find_attribute_directions(self, latent_codes, attributes, attribute_names):\n        \"\"\"\n        Find directions in latent space corresponding to attributes\n        \"\"\"\n        directions = {}\n        \n        for i, attr_name in enumerate(attribute_names):\n            # Split samples by attribute value\n            pos_samples = latent_codes[attributes[:, i] == 1]\n            neg_samples = latent_codes[attributes[:, i] == 0]\n            \n            if len(pos_samples) > 0 and len(neg_samples) > 0:\n                # Compute direction as difference in means\n                direction = np.mean(pos_samples, axis=0) - np.mean(neg_samples, axis=0)\n                direction = direction / np.linalg.norm(direction)  # Normalize\n                \n                directions[attr_name] = direction\n                \n                print(f\"{attr_name}: Found direction vector\")\n            else:\n                print(f\"{attr_name}: Insufficient samples\")\n        \n        return directions\n    \n    def manipulate_attributes(self, input_image, directions, attribute_names, \n                            manipulation_strengths=None):\n        \"\"\"\n        Manipulate specific attributes in an input image\n        \n        Args:\n            input_image: Input image tensor [1, C, H, W]\n            directions: Dictionary of attribute direction vectors\n            attribute_names: List of attributes to manipulate\n            manipulation_strengths: Strength of manipulation for each attribute\n        \"\"\"\n        if manipulation_strengths is None:\n            manipulation_strengths = [3.0] * len(attribute_names)\n        \n        # Encode input image\n        with torch.no_grad():\n            input_image = input_image.to(self.device)\n            z_original, _, _ = self.vae.encode(input_image)\n            z_original = z_original.cpu().numpy()\n        \n        results = {'original': input_image}\n        \n        # Manipulate each attribute\n        for attr_name, strength in zip(attribute_names, manipulation_strengths):\n            if attr_name in directions:\n                direction = directions[attr_name]\n                \n                # Add direction to latent code\n                z_modified = z_original + strength * direction.reshape(1, -1)\n                \n                # Decode modified latent code\n                with torch.no_grad():\n                    z_tensor = torch.FloatTensor(z_modified).to(self.device)\n                    modified_image = self.vae.decoder(z_tensor)\n                \n                results[f'{attr_name}_+{strength}'] = modified_image\n                \n                # Also try negative direction\n                z_modified_neg = z_original - strength * direction.reshape(1, -1)\n                \n                with torch.no_grad():\n                    z_tensor_neg = torch.FloatTensor(z_modified_neg).to(self.device)\n                    modified_image_neg = self.vae.decoder(z_tensor_neg)\n                \n                results[f'{attr_name}_-{strength}'] = modified_image_neg\n        \n        return results\n    \n    def interpolate_attributes(self, image1, image2, num_steps=10):\n        \"\"\"\n        Interpolate between two images in latent space\n        \"\"\"\n        with torch.no_grad():\n            image1 = image1.to(self.device)\n            image2 = image2.to(self.device)\n            \n            z1, _, _ = self.vae.encode(image1)\n            z2, _, _ = self.vae.encode(image2)\n            \n            interpolated_images = []\n            \n            for step in range(num_steps):\n                alpha = step / (num_steps - 1)\n                z_interp = (1 - alpha) * z1 + alpha * z2\n                \n                img_interp = self.vae.decoder(z_interp)\n                interpolated_images.append(img_interp)\n        \n        return interpolated_images\n    \n    def visualize_manipulations(self, manipulation_results, save_path=None):\n        \"\"\"\n        Visualize attribute manipulation results\n        \"\"\"\n        num_results = len(manipulation_results)\n        fig, axes = plt.subplots(1, num_results, figsize=(num_results * 3, 3))\n        \n        if num_results == 1:\n            axes = [axes]\n        \n        def denormalize(x):\n            return torch.clamp((x + 1) / 2, 0, 1)\n        \n        for i, (title, image) in enumerate(manipulation_results.items()):\n            if isinstance(image, torch.Tensor):\n                img_np = denormalize(image[0]).cpu().permute(1, 2, 0).numpy()\n                axes[i].imshow(img_np)\n            axes[i].set_title(title)\n            axes[i].axis('off')\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        plt.show()\n\ndef evaluate_disentanglement(vae_model, dataloader, attribute_names, device='cuda'):\n    \"\"\"\n    Comprehensive evaluation of disentanglement quality\n    \"\"\"\n    manipulator = AttributeManipulator(vae_model, device)\n    \n    # Encode dataset\n    print(\"Encoding dataset...\")\n    latent_codes, attributes = manipulator.encode_dataset(dataloader, max_samples=2000)\n    \n    # Train attribute classifiers\n    manipulator.train_attribute_classifiers(latent_codes, attributes, attribute_names)\n    \n    # Find attribute directions\n    directions = manipulator.find_attribute_directions(latent_codes, attributes, attribute_names)\n    \n    # Compute disentanglement metrics\n    metrics = compute_disentanglement_metrics(latent_codes, attributes, attribute_names)\n    \n    return manipulator, directions, metrics\n\ndef compute_disentanglement_metrics(latent_codes, attributes, attribute_names):\n    \"\"\"\n    Compute disentanglement metrics (SAP, MIG, etc.)\n    \"\"\"\n    metrics = {}\n    \n    # Compute mutual information between latent dimensions and attributes\n    mutual_info_matrix = np.zeros((latent_codes.shape[1], len(attribute_names)))\n    \n    for i in range(latent_codes.shape[1]):  # For each latent dimension\n        for j, attr_name in enumerate(attribute_names):  # For each attribute\n            # Discretize latent dimension\n            latent_discrete = np.digitize(latent_codes[:, i], \n                                        bins=np.percentile(latent_codes[:, i], \n                                                         [20, 40, 60, 80]))\n            \n            # Compute mutual information (simplified)\n            mi = compute_mutual_information(latent_discrete, attributes[:, j].astype(int))\n            mutual_info_matrix[i, j] = mi\n    \n    # SAP Score (Separated Attribute Predictability)\n    sap_scores = []\n    for j in range(len(attribute_names)):\n        mi_j = mutual_info_matrix[:, j]\n        if len(mi_j) > 1:\n            sorted_mi = np.sort(mi_j)[::-1]  # Sort descending\n            if sorted_mi[1] > 0:\n                sap = (sorted_mi[0] - sorted_mi[1]) / sorted_mi[0]\n            else:\n                sap = sorted_mi[0] if sorted_mi[0] > 0 else 0\n            sap_scores.append(sap)\n    \n    metrics['SAP'] = np.mean(sap_scores) if sap_scores else 0\n    metrics['MI_Matrix'] = mutual_info_matrix\n    \n    # Modularity score\n    modularity_scores = []\n    for i in range(latent_codes.shape[1]):\n        mi_i = mutual_info_matrix[i, :]\n        if np.sum(mi_i) > 0:\n            modularity = np.max(mi_i) / np.sum(mi_i)\n            modularity_scores.append(modularity)\n    \n    metrics['Modularity'] = np.mean(modularity_scores) if modularity_scores else 0\n    \n    return metrics\n\ndef compute_mutual_information(x, y):\n    \"\"\"\n    Compute mutual information between discrete variables x and y\n    \"\"\"\n    # Create contingency table\n    unique_x = np.unique(x)\n    unique_y = np.unique(y)\n    \n    if len(unique_x) <= 1 or len(unique_y) <= 1:\n        return 0.0\n    \n    contingency = np.zeros((len(unique_x), len(unique_y)))\n    \n    for i, val_x in enumerate(unique_x):\n        for j, val_y in enumerate(unique_y):\n            contingency[i, j] = np.sum((x == val_x) & (y == val_y))\n    \n    # Normalize to get probabilities\n    p_xy = contingency / np.sum(contingency)\n    p_x = np.sum(p_xy, axis=1)\n    p_y = np.sum(p_xy, axis=0)\n    \n    # Compute mutual information\n    mi = 0.0\n    for i in range(len(unique_x)):\n        for j in range(len(unique_y)):\n            if p_xy[i, j] > 0 and p_x[i] > 0 and p_y[j] > 0:\n                mi += p_xy[i, j] * np.log(p_xy[i, j] / (p_x[i] * p_y[j]))\n    \n    return mi\n\n# Example usage:\n\n# After training your VAE\nmanipulator, directions, metrics = evaluate_disentanglement(\n    trained_vae, val_loader, SELECTED_ATTRS, device\n)\n\nprint(\"Disentanglement Metrics:\")\nprint(f\"SAP Score: {metrics['SAP']:.3f}\")\nprint(f\"Modularity: {metrics['Modularity']:.3f}\")\n\n# Test attribute manipulation\nsample_batch = next(iter(val_loader))\ntest_image = sample_batch[0][:1]  # First image\n\nmanipulation_results = manipulator.manipulate_attributes(\n    test_image, \n    directions, \n    ['Male', 'Smiling', 'Eyeglasses'], \n    manipulation_strengths=[3.0, 2.5, 3.5]\n)\n\nmanipulator.visualize_manipulations(manipulation_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}